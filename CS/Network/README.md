# Network

  - [1. 인터넷 네트워크](#1-인터넷-네트워크)
    - 계층구조
      - 계층 구조가 필요한 이유
    - 인터넷 프로토콜 스택
    - OSI 모델
  - [2. 애플리케이션 계층](#2-애플리케이션-계층)
    - 애플리케이션 계층의 원리
    - DNS란?
    - DNS의 동작원리
    - P2P
  - [3. 트랜스포트 계층](#3-트랜스포트-계층)
    - 트랜스포트 계층의 역할
    - UDP와 TCP의 차이
    - TCP의 신뢰적인 데이터 전송이란?
    - 다중화와 역다중화
    - UDP
    - TCP
    - TCP는 GBN인가? SR인가?
    - TCP 3 way handshake
    - TCP 4 way handshake
    - TCP 흐름제어
    - TCP 혼잡제어
  - [4. 네트워크 계층-데이터 평면](#4-네트워크-계층-데이터-평면)
    - 포워딩과 라우팅
    - 라우터 내부 구조
    - 패킷 스케줄링
    - 서브넷이란?
    - DHCP(Dynamic Host Configuration Protocol)
    - NAT


</br>

## 1. 인터넷 네트워크
### 계층구조 
#### 계층 구조가 필요한 이유
인터넷은 매우 복잡한 시스템이다.  
다양한 애플리케이션과 프로토콜, 여러 가지 종단 시스템과 종단 시스템 간의 연결, 라우터, 다양한 링크 수준의 매체가 존재한다. 인터넷이 이렇게 아주 복잡하다면, 네트워크 구조를 어떻게 조직하는가?를 먼저 알아야 한다.  
#### 계층 구조의 장점
계층 구조는 크고 복잡한 시스템의 잘 정의된 특정 부분을 논의할 수있게 해주고, 이러한 단순화는 매우 중요하다.  
시스템이 계층 구조를 가지면, 그 계층이 제공하는 서비스의 구현을 변경하는 것도 매우 쉬워진다. 한 계층이 상위 계층에 같은 서비스를 제공하고, 하위 계층의 서비스를 이용하는 한, 어떤 한 계층의 구현이 변하더라도 시스템의 나머지 부분은 변하지 않기 때문이다. 

### 인터넷 프로토콜 스택
인터넷 프로토콜 스택은 5계층으로 구성된다. top-down 방식으로 접근해보면, 아래의 순서와 같다.
1. 애플리케이션 계층  
  애플리케이션 계층 프로토콜은 여러 종단 시스템에 분산되어 있다.  
  한 종단 시스템에 있는 애플리케이션이 다른 종단 시스템에 있는 애플리케이션과 정보 패킷을 교환할때 이 프로토콜을 사용한다.  

2. 트랜스포트 계층  
  트랜스포트 계층은 클라이언트와 서버 간에 애플리케이션 계층 메시지를 전송하는 서비스를 제공한다.  
  인터넷에는 TCP와 UDP라는 두가지 트랜스포트 프로토콜이 존재하고, 이들은 애플리케이션 계층 메시지를 전달한다.  

3. 네트워크 계층  
  네트워크 계층은 한 호스트에서 다른 호스트로 데이터그램을 라우팅하는 책임을 가지며, 목적지 호스트의 트랜스포트 계층으로 세그먼트를 운반하는 서비스를 제공한다.  
  IP 프로토콜이 네트워크 계층에 속하며, 다른 프로토콜은 존재하지 않기 떄문에 네트워크 계층을 가진 모든 인터넷 요소는 IP 프로토콜을 수행해야만 한다.  

4. 링크 계층  
  네트워크 계층은 출발지와 목적지 간 일련의 패킷 스위치를 통해 데이터그램을 라우트한다. 이때 네트워크 계층은 링크 계층 서비스에 의존한다.  
  링크 계층은 네트워크 계층의 데이터그램을 받아 경로상의 다음 노드로 전달한다. 다음 노드에 존재하는 링크 계층은 이 데이터그램을 상위 네트워크 계층으로 보낸다.

5. 물리 계층  
  물리 계층은 링크 계층의 프레임 내부의 각 비트를 한 노드에서 다음 노드로 전달시킨다.  
  이 계층의 프로토콜들은 실제 물리 전송 매체에 의존하며, 각 매체별로 비트는 다른 방식으로 반대편으로 이동한다.

### OSI 모델
OSI 모델은 인터넷 프로토콜 모델과 다르다. 그리고 이것은 인터넷 프로토콜이 유일한 프로토콜 스택이 아니라는 것을 알 수있게 해준다. ISO가 제안한 컴퓨터 네트워크가 7계층으로 구성되어야 한다는 제안이 OSI 7계층이고, 인터넷 프로토콜 모델의 틀로 사용되었다. OSI 7계층은 아래와 같이 구성되어 있다.
1. 애플리케이션 계층 
2. 프레젠테이션 계층 
3. 세션 계층 
4. 트랜스포트 계층 
5. 네트워크 계층 
6. 데이터 링크 계층
7. 물리 계층

인터넷 계층과의 차이는 프레젠테이션 계층, 세션 계층이다.  

<br>

## 2. 애플리케이션 계층
### 애플리케이션 계층의 원리
네트워크에서 애플리케이션 계층은 컴퓨터 네트워크가 존재하는 이유이다. 
대표적으로는 웹, P2P 파일 공유, VoIP, 게임, SMTP 등이 있다. 우리가 개발하는 웹이나 앱도 애플리케이션 계층에 속한다.  
<br>
중요한 것은 새로운 애플리케이션을 개발할때, 라우터나 링크 계층 스위치와 같이 네트워크 코어 장비에서 실행되는 소프트웨어를 작성할 필요가 없다는 점이다. 개발자는 네트워크 구조가 고정되어 있고, 애플리케이션에 특정 서비스 집합을 제공하기만 하면 된다. 이렇게 종단 시스템에만 애플리케이션 소프트웨어가 존재한다는 기본 설계는 인터넷 애플리케이션이 광대하고 빠르게 발전할 수있던 원동력이 되었다.

<br>
현대 네트워크 애플리케이션에서 사용되는 구조는 'Client-Server' 혹은 'P2P'이다. 이때 발생하는 통신은 운영체제 관점에서 실제 통신하는 것은 '프로세스'이다. 프로세스는 컴퓨터 네트워크를 통해 '메시지'를 교환하고 서로 통신한다.  
  
<br>
<br>
한대의 컴퓨터에는 여러 프로세스가 동작할 수있다. 따라서 우리는 각 메시지가 어떻게 특정 프로세스를 위한 메시지인지 판별할 수있는가?를 이해해야 한다. 
이는 '소켓'을 통해 가능하다. 각 프로세스는 '소켓'을 통해 네트워크로 메시지를 보내고 받을 수있다. 따라서 소켓이 애플리케이션 계층과 트랜스포트 계층 간의 인터페이스라는 것을 알 수있다.     
<br>
<br>
IP 주소를 이용해 Server 컴퓨터로 메시지가 들어왔다고 가정을 해보자. 그러면 소켓은 이 메시지를 적절한 프로세스로 전달해야 한다. 그리고 이때 프로세스의 'Port' 번호가 사용된다. 소켓은 Port 번호를 이용해 수신 프로세스를 식별한다. 

<br>
<br>

### DNS란?
DNS는 대표적인 애플리케이션 계층에 속하는 애플리케이션이다. 우리는 자주 이용하는 웹사이트를 Domain Name을 활용해 접속하곤 한다. 이 웹사이트에 request를 보내는 방법은 이것뿐일까? 아니다. 해당 Domain Name과 매핑된 IP 주소로도 request를 보내 웹사이트에 접속할 수있다.

이 두가지 방법 중 Domain Name을 사용해서 웹사이트에 접속하는 이유는 간단하다. 더 쉽기 때문이다. 사용자에게는 의미없어 보이는 IP 주소를 외우기란 쉽지가 않다. 또한 IP 주소가 변경되는 경우도 많다.  

하지만, 컴퓨터의 입장에서 생각해보면, 이 문제는 그리 단순하지 않다. Domain Name은 서버 컴퓨터의 위치에 대한 정보를 숨기고 있다. 이러한 이유로 라우터는 목적지 주소로 Domain Name을 받으면 어디로 보낼지 혼란스러워 한다. 

사용자와 컴퓨터의 패러다임 충돌로 인해 DNS가 필요하다. 사용자는 Domain Name으로 Server를 찾기 희망하고, 컴퓨터는 IP 주소를 더 좋아한다. 이 두개의 식별자를 매핑하여 원하는 정보를 알려주는 것이 바로 DNS가 하는 역할이다. 

### DNS의 동작원리
DNS는 분산형 데이터베이스라고 볼 수 있다. 왜? DNS는 분산형으로 설계가 되었을까? 이 의문을 해결하는 방법은 간단하다. 중앙 집중형으로 DNS를 설계했을때 문제점이 무엇인지 이해하면 된다. 

<br> 

요즘은 정말 많은 Domain Name이 있다. 그리고 DNS는 이 Domain Name을 IP 주소로 변환하는 역할을 수행한다. 이 매핑 관계는 DNS의 Database에 저장될 것이다. 만약 하나의 DNS 서버가 모든 매핑 정보를 저장한다면 어떤일이 생길까?   

1. 서버의 고장 : 한대의 DNS 서버가 고장나면 모든 서비스가 중단된다.
2. 트래픽 양 : 한대의 DNS 서버가 모든 트래픽을 받고 처리해야 한다.
3. RTT : 물리적인 위치에 따라 DNS의 응답을 받는 시간이 달라질 수있다.
4. 유지관리 : 하나의 DB에 모든 레코드를 저장하면 DB는 매우 거대해지고 사용자를 등록할때마다 DB를 갱신해야 한다. 

위와같은 이유가 대표적이고, 한마디로 정리하면 '확장성'이 매우 떨어진다.
<br>
이러한 이유들로 DNS는 분산형으로 설계되었다. 어떠한 단일 DNS 서버도 인터넷에 있는 모든 호스트에 대한 매핑을 갖지 않는 대신에 DNS 서버 사이에 분산된다. DNS는 크게 3가지 유형이 존재한다.
<br>
1. 루트 DNS 서버
2. TLD DNS 서버
3. 책임 DNS 서버

client가 어떤 Domain Name과 매핑되는 IP 주소를 얻고 싶다고 가정하자. 이때 DNS 서버로 Domain Name을 담은 패킷을 전송하는데, 위의 유형의 순서대로 탐색을 진행한다. 루트 DNS 서버는 domain name을 분석해 매핑된 TLD DNS의 IP를 찾아 client에게 알려준다. client는 이제 요청을 TLD DNS에게 다시 보낸다. TLD는 domain name을 분석해 책임 DNS 서버의 IP 주소를 client에게 전송한다. client는 책임 DNS 서버에게 다시 domain name에 대해 물어보고, 책임 DNS 서버는 자신이 가지고 있던 정보를 활용해 서버의 IP 주소를 client에게 알려준다.

### P2P
P2P는 Client-Server 구조와 매우 다르다. 
<br>
대부분의 애플리케이션은 항상 켜져있는 기반 구조 서버에 상당히 의존하는 Client-Server 구조를 채택한다. 반면 P2P 구조는 항상 켜져 있는 기반구조 서버에 최소한으로 의존한다는 특징을 가진다. 대신, 호스트들이 연결되어 서로 직접 통신하는 구조이다.  
P2P 구조를 선택함으로써 얻을 수있는 장점은 서버에게 커다란 부하를 주지 않고, 서버의 분배 프로세스를 도울 수있다.

## 3. 트랜스포트 계층
### 트랜스포트 계층의 역할
애플리케이션 계층과 네트워크 계층 사이에 존재하는 트랜스포트 계층은 네트워크 구조의 핵심이다. 트랜스포트 계층을 아주 간단하게 정리하면 서로 다른 호스트에서 동작하는 애플리케이션 프로세스에게 직접 통신 서비스를 제공하는 역할을 수행한다.   
네트워크 계층 프로토콜은 호스트들 사이의 논리적 통신을 제공하는 반면, 트랜스포트 계층은 프로세스들 사이의 논리적 통신을 제공하는 차이는 매우 중요하다.
<br>
<br>
또한 트랜스포트 계층 프로토콜은 네트워크 라우터가 아닌 종단 시스템에서 구현된다.

### UDP와 TCP
UDP는 비신뢰적이고 비연결형인 서비스를 제공하는 반면, TCP는 신뢰적이고 연결형 서비스를 제공한다.

이 둘의 가장 기본적인 기능은 종단 시스템 사이의 IP 전달 서비스를 종단 시스템에서 동작하는 두 프로세스 간의 전달 서비스로 확장하는 것이다.  
또 다른 공통점은 헤더에 오류 검출 필드를 포함함으로써 무결성 검사를 제공한다는 것이 있다. 이러한 최소한의 두가지 트랜스포트 계층 서비스는 UDP가 제공하는 유일한 두가지 서비스이다. 

<br>
반면 TCP는 몇가지 추가적인 서비스를 제공하는데, 대표적으로 신뢰적인 데이터 전달을 제공한다. 또 흐름제어와 혼잡제어는 UDP와 TCP를 구별하는 대표적인 특징이다.

<br>
<br>

### TCP의 신뢰적인 데이터 전송이란?
송신하는 프로세스가 보낸 데이터를 수신하는 프로세스가 정확하게 받는 것을 보장하며, 패킷의 순서도 보장된다.  
<br>


### 다중화와 역다중화
다중화, 역다중화는 네트워크 계층이 제공하는 '호스트-호스트' 전달 서비스를 '프로세스-프로세스' 전달 서비스로 확장하는 것이며 모든 컴퓨터 네트워크에서 필요하다.  
<br>
다중화와 역다중화는 두 가지 요구사항을 가지고 있다.  
1. 소켓은 유일한 식별자를 가진다.  
2. 세그먼트는 전달될 적절한 소켓을 가리키는 특별한 필드를 가진다.  

여기서 등장한 소켓은 네트워크 -> 프로세스, 프로세스 -> 네트워크로 데이터를 전달하는 출입구이다.  
트랜스포트 계층은 데이터를 직접 프로세스로 전달하지 않고 소켓에게 대신 전달한다.  
세그먼트의 필드는 어떤 소켓으로 데이터를 전달해야 하는지에 대한 정보를 가지고 있는 식별자이다.  

TCP와 UDP는 다중화 역다중화 과정도 다르다.  
UDP는 도착지 IP 주소, 포트 번호로 소켓을 식별하는 반면, TCP는 도착지, 출발지 IP 주소, 도착지, 출발지 포트 번호로 소켓을 식별한다. 

### UDP
UDP는 트랜스포트 계층 프로토콜이 할 수있는 최소 기능으로 동작한다. 최소 기능은 아래와 같다.  
1. 다중화/역다중화
2. 간단한 오류검사

이 둘을 제외하면 UDP는 IP에 아무것도 추가하지 않는다.  
UDP는 3-way handshake 과정이 없기 때문에 TCP 보다 빠르다. 그리고 이 핸드셰이크 과정의 공백이 UDP를 '비연결형'으로 만들어준다.  

UDP와 TCP 중 어느것이 더 뛰어나다고 말할 수없고, UDP의 특징은 아래와 같다.  

1. 애플리케이션 레벨에서 더 정교한 제어가 가능하다.
2. 연결 설정이 없다.
3. 연결 상태가 없다
4. 작은 패킷 오버헤드

UDP 세그먼트의 구조는 아래의 그림과 같다. 
<br>  

<img width="408" alt="image" src="https://user-images.githubusercontent.com/76645095/179386341-14748ea3-b340-4c7f-bd00-acb3f9e8c7f1.png">
<br>  
<br>  
UDP 헤더는 2바이트씩 구성된 4개의 필드를 가진다. 이 필드들은 앞서 언급했던 트랜스포트 계층의 최소 기능을 수행하기 위해 존재하는 필드이다.

여기서 포트 번호는 정확한 프로세스에게 애플리케이션 데이터를 넘기게 해주고, 체크섬은 세그먼트에 오류가 발생했는지를 검사하기 위해 수신 호스트에서 사용된다.  

### TCP
TCP는 신뢰적인 데이터 전송을 보장한다.  
신뢰적인 데이터 전송을 위해 TCP는 오류 검출, 재전송, 누적 확인응답, 타이머, 순서번호, 확인응답 번호를 위한 헤더 필드를 가진다.  

TCP는 프로세스가 데이터를 다른 프로세스에게 보내기전에 두 프로세스가 서로 '핸드셰이크'를 먼저 진행해야 하므로 '연결 지향형'의 특징을 가진다.  
핸드셰이크는 TCP 연결 설정의 일부로서, 연결된 종단은 TCP 연결과 연관된 많은 TCP 상태 변수를 초기화한다.  

주의할 것은 TCP 프로토콜은 오직 종단 시스템에서만 동작하고, 중간의 네트워크 요소에서는 동작하지 않으므로, 중간의 네트워크 요소들은 TCP 연결 상태를 유지하지 않는다.  
애초에 중간 라우터들은 네트워크 계층이기 때문에 TCP 연결을 감지조차 못한다.  

TCP 세그먼트 구조는 아래의 그림과 같다.  

<img width="861" alt="image" src="https://user-images.githubusercontent.com/76645095/179387217-ea7a8334-5aa9-4a3b-90c3-03468d27cf06.png">

<br>  
TCP 세그먼트는 헤더 필드와 데이터 필드로 구성되어 있다.  

<br>

TCP 헤더는 일반적으로 20바이트 이고, UDP 헤더보다 12바이트 크다.
<br>  
UDP처럼 헤더는 상위 계층 애플리케이션으로부터 다중화와 역다중화를 하는데 사용하는 '출발지'와 '목적지' 포트 번호를 포함한다.  
또한 UDP처럼 체크섬 필드도 포함하고 있는 것을 볼 수있다. 추가적인 필드는 아래와 같은 것들이 존재한다.  

- 순서번호 필드 + 확인 응답번호 필드 : 신뢰적인 데이터 전송 서비스 구현에서 TCP 송신자와 수신자에 의해서 사용된다.  
- 수신 윈도우 : 흐름제어를 위해 사용된다.  
- 헤더 길이 : TCP 헤더의 길이를 나타낸다. 
- 옵션 필드 : 송신자와 수신자가 최대 세그먼트 크기(MSS)를 협상하기 위해 사용된다. 
- 플래그 비트 : ACK, SYN, FIN, PSH 비트 등을 포함한다.

### TCP는 GBN인가? SR인가?

이 의문을 해결하기 위해서는 GBN과 SR에 대한 지식이 필요하다.
간략하게 이 둘은 네트워크에서 신뢰적인 전송을 위한 2가지 파이프라인 프로토콜이다.

rdt의 고전적인 문제는 stop-and-wait 방식을 사용하는 것인데, 패킷을 하나씩 보내고 확인응답을 받고 하며 동작하기 때문에 매우 성능이 낮다.

따라서 이 문제를 해결하기 위해 한번에 여러개의 패킷을 전송하는 파이프라인 프로토콜이 등장했고, 두가지 기법이 GBN, SR이다.

<br>
이 둘도 차이가 존재하는데, ACK의 의미가 우선 다르다.  
<br>
GBN의 ACK N은 seq N까지의 패킷을 받았다는 의미를 가진다. 또한 슬라이딩 윈도우 기법을 활용하며 하나의 패킷이라도 유실된다면 유지하고 있는 window size의 모든 패킷을 재전송한다. 때문에 이미 수신자 입장에서 받은 패킷도 재전송하는 문제가 발생하고, 그만큼 성능이 떨어진다.  

<br>
<br>

SR은 이 문제를 해결하기 위해 선택적으로 패킷에 대한 ACK를 전송하고, ACK N의 의미는 N번 패킷을 잘 받았다는 의미를 가진다. 또한 수신 버퍼를 유지하면서 이전에 받은 패킷을 저장한다.

만약 N번 패킷에 대해 timeout 이벤트가 발생한다면, SR은 딱 N번 패킷만 보낸다. 모든 패킷을 재전송하는 GBN과의 차이가 이부분이다.

<br>
그래서 TCP가 GBN인가? SR인가? 해결해야 한다.

결론부터 말하면, TCP는 GBN과 SR의 특징을 모두 활용하는 혼합된 형태이다.

<br>

TCP는 GBN의 ACK 방식을 활용하면서, SR의 수신자와 같이 수신 버퍼를 유지한다.


예를들어 송신자가 N과 N+1 패킷을 전송했다고 가정해보자.

송신자는 패킷을 보내고 타이머를 동작시킬 것이다. 이 패킷 N은 정상적으로 수신자에게 도착했지만, ACK N이 유실되었다. 그리고 수신자는 N+1 패킷도 받고, ACK N+1을 송신자에게 보낸다.


송신자는 이 상황에서 ACK N을 절대로 받지 못한다. 그런데 N번 패킷에 대한 timeout이 발생하기전 ACK N+1을 받는다면, TCP에서 송신자는 수신자가 N번 패킷까지 받았다고 판단한다. 그리고 N+2 패킷을 보낸다. 이는 GBN의 ACK 방식과 매우 유사하다.


만약 위의 상황과 달리 패킷 N에 대한 timeout이 발생했다면? GBN 방식대로라면 다시 모든 패킷을 전송할 것이다. 하지만, TCP는 딱 패킷 N만 다시 재전송하고, 수신자도 버퍼를 유지하고 있기에 정상적으로 모든 패킷을 받는다. 이는 SR의 동작 방식과 매우 유사해보인다.  
TCP는 GBN의 ACK 응답 방식을 사용하고, SR의 버퍼링 방식을 같이 사용하는 혼종이다. 왜? TCP는 이런 방식을 사용할까?  
<br>

GBN의 단점은 수신 버퍼가 존재하지 않아 모든 패킷을 보낸다는 것이고, SR의 단점은 모든 ACK 패킷을 확인한다는 것이다. 특히, SR은 수신자 입장에서 분명 ACK를 보냈는데 유실되면서 다시 ACK를 재전송해야 하는 문제가 생긴다. 이때 GBN을 결합하면 불필요한 패킷 재전송을 수행하지 않을 수있기 때문에, 이 둘을 적절히 혼합하면서 불필요한 패킷 재전송을 더욱 줄일 수있는 것이다.

<br>


### TCP 3 way handshake
TCP 3 way handshake는 client-server간 TCP 연결을 수행하는 과정이다.  
TCP 연결이 어떻게 설정 되는지 살펴보기 위해 client에서 운영되는 프로세스가 server안의 프로세스와 연결을 초기화하길 원한다고 가정하자.  
<br> 
먼저 client 애플리케이션 프로세스는 서버에 있는 프로세스와 연결 설정하기를 원한다는 것을 client TCP에게 알린다.  
그러면, client 안의 TCP는 아래의 그림과 같은 방법으로 서버와 TCP 연결 설정을 시작하며, 3개의 패킷을 주고받기 때문에 3-way-handshake라고 불린다.  

![image](https://user-images.githubusercontent.com/76645095/179393940-5829740f-2025-4614-8889-d3d29f8e8f60.png)
<br> 

1. 먼저 client TCP는 server TCP에게 특별한 TCP 세그먼트를 송신한다.   
이 세그먼트에는 애플리케이션 계층 데이터가 포함되지 않으며, TCP 세그먼트 헤더의 SYN 플래그 비트를 1로 설정한다.  
그리고 추가로 최초의 Client sequence number를 선택하여 세그먼트 순서번호 필드에 이 번호를 넣는다.  
이 세그먼트는 IP 데이터그램 안에서 캡슐화되고, 서버로 전송된다.
2. TCP SYN 세그먼트를 포함하는 IP 데이터그램이 서버 호스트에 도착했을때, 서버는 데이터그램으로부터 TCP SYN 세그먼트를 뽑아낸다.  
그리고 서버 호스트는 연결에 사용될 '버퍼'와 '변수'들을 할당한다.  
또한 client TCP로 연결 승인 세그먼트를 보내는데, 이 연결 승인 세그먼트도 애플리케이션 계층 데이터를 포함하지 않으며 SYN 비트를 1로 설정한 뒤, ACK 필드는 클라이언트 세그먼트 순서번호 +1을, 설정하고, Server sequence number를 선택하여 세그먼트 순서번호 필드에 넣는다.  
이 세그먼트를 SYNACK라고 부르기도 한다.
3. SYNACK 세그먼트를 클라이언트가 수신하면, 클라이언트는 연결에 사용될 '버퍼'와 '변수'를 할당한다. 그리고 또 다른 세그먼트를 서버로 송신한다.  
이제 연결이 설정된 것이고, SYN 비트는 0으로 설정한다.  
이때 3번째 패킷 payload에는 데이터를 넣을 수도 있다.  
<br> 


이 세 단계가 완료되면, 클라이언트와 서버 호스트들은 각각 서로에게 데이터를 포함하는 세그먼트를 보낼 수있다. 그리고 이제부터 주고받는 세그먼트들의 SYN 비트는 0으로 설정된다.  
<br> 


### TCP 4 way handshake
<br> 
TCP 연결에 참여하는 2개의 프로세스 중 하나는 연결을 끊어낼 수있다. 연결이 끝날때, 할당된 호스트 자원 (버퍼, 변수)는 회수된다.  

아래의 그림은 클라이언트가 연결을 종료한다고 가정한 그림이다. 그림을 보고 이해해보자.
<br> 
<br> 

![image](https://user-images.githubusercontent.com/76645095/179394273-fb300039-2600-473d-8c32-74fe8eb97a9e.png)

<br> 

1. client 애플리케이션 프로세스는 종료 명령을 내리고, client TCP가 server 프로세스에게 특별한 TCP 세그먼트를 보낸다.  
이 세그먼트는 FIN 비트가 1로 설정된 세그먼트 헤더를 가지고 있다. 
2. Server가 세그먼트를 수신하면, client에게 확인 세그먼트를 보낸다. 
3. Server는 FIN 비트가 1로 설정된 server의 종료 세그먼트를 연속으로 보낸다.  
4. client는 server의 종료 세그먼트에 대한 응답 세그먼트를 보내며, 이 시점에서 두 호스트의 모든 자원들은 할당이 해제된다.  

<br>


### TCP 흐름제어
먼저 TCP는 흐름제어를 제공하고, UDP는 흐름제어를 제공하지 않는 점은 이 둘의 큰 차이 중 하나이다.  
TCP 연결의 각 종단에서 호스트들은 연결에 대한 개별 수신 버퍼를 설정한다. TCP 연결이 순서대로 올바르게 바이트를 수신할 때, TCP는 데이터를 수신 버퍼에 차곡차곡 저장한다.  
<br>
하지만, 이 버퍼의 크기는 한정적이다. 따라서 수신자가 버퍼를 읽는 속도가 송신자가 데이터를 전송하는 속도보다 느리다면, 수신 버퍼는 오버플로우가 발생할 것이다.
<br>
이렇게 송신자가 수신자의 버퍼를 오버플로우 시키는 것을 방지하기 위해서 TCP는 '흐름제어 서비스(flow-control service)'를 제공한다.  
<br>
이는 송신자의 데이터 전송 속도와 수신자가 버퍼를 읽는 속도를 일치시키는 목적을 가진 서비스이다.  
<br>
흐름제어와 혼잡제어의 목적은 분명히 다르지만, 동작하는 방식은 비슷하다.  
먼저 TCP는 흐름제어를 어떻게 제공하는지 살펴보자.  
<br> 

<img width="861" alt="image" src="https://user-images.githubusercontent.com/76645095/179387217-ea7a8334-5aa9-4a3b-90c3-03468d27cf06.png">

<br> 
위의 그림은 TCP 세그먼트의 구조이다.  
여기서 '수신 윈도우'는 수신 측에서 사용 가능한 버퍼 공간이 얼마나 되는지 송신자에게 알려주는 필드이고, 흐름제어에서 사용되는 필드이다.  
<br> 

송신자는 이 필드를 이용해 수신자에게 얼마의 버퍼 공간이 남았는지 계산할 수있다. 따라서 수신 버퍼에 overflow가 발생하기까지 얼마나 남았는지 확신할 수있고, 데이터 전송률을 조절한다.  
<br> 

하지만, 이 방법에는 문제가 있다. 만약 수신자의 남은 버퍼 공간이 0이라면? 송신자는 더이상 데이터를 보내지 않을 것이다. 
<br> 
송신자가 패킷을 보내지 않으면, 수신자의 버퍼 공간에 여유가 생긴다해도 송신자는 이 사실을 알 방법이 없다.  
<br> 
이 문제를 해결하기 위해 TCP 명세서는 수신자의 수신 윈도우가 0일때, 송신자는 1바이트의 더미 데이터를 계속해서 전송하도록 요구한다.  
그리고 송신자는 이 더미데이터에 들어있는 수신 윈도우의 값으로 수신자의 버퍼 공간을 확인한다.  


### TCP 혼잡제어
혼잡제어 매커니즘은 TCP의 또 다른 매우 중요한 요소이다.  
IP 계층은 네트워크 혼잡에 관해서 종단 시스템에게 어떠한 직접적인 피드백도 제공하지 않는다. 따라서 TCP는 네트워크 지원 혼잡제어보다는 '종단간'의 혼잡제어를 사용해야 한다.  
<br>
TCP가 취한 접근방법은 네트워크 혼잡에 따라 연결에 트래픽을 보내는 전송률을 각 송신자가 제한하도록 하는 것이다.  
만약 TCP 송신자가 자신과 목적지 간의 경로에서 혼잡이 없음을 감지하면 송신자는 송신율을 높인다.   
반면 송신자가 경로 사이에 혼잡을 감지하면, 송신율을 높인다. 이것은 3가지 해결해야 하는 과제를 주는데, 아래와 같은 것들이 있다.  
<br>

1. TCP 송신자는 자신의 연결에 전송 트래픽 전송률을 어떻게 제한하는가?  
2. TCP 송신자는 자신과 목적지 사이 경로의 혼잡을 어떻게 감지하는가?
3. 송신자는 종단간의 혼잡을 감지함에 따라 송신율을 변화시키기 위해서 어떤 알고리즘을 사용해야하는가?  

<br>

먼저 TCP 송신자가 연결로 트래픽을 보내는 전송률을 어떻게 제한하는지 해결해보자.  
TCP 연결의 양 끝 각 호스트들은 수신 버퍼, 송신 버퍼 그리고 몇 가지의 변수로 구성된다. 그리고 송신 측에서 동작하는 TCP 혼잡제어 메커니즘은 추가적인 변수인 '혼잡 윈도우(congestion window)'를 기록한다.  
송신자는 이 혼잡 윈도우(cwnd)를 조절하여, 링크에 데이터를 전송하는 비율을 조절한다.  

<br>

그러면, TCP 송신자는 자신과 목적지 사이의 경로에 혼잡이 존재하는지를 어떻게 감지하는가?    
네트워크에 과도한 혼잡이 발생하면, 경로에 있는 하나 이상의 라우터 버퍼들이 overflow되고, 그 결과 데이터그램이 버려진다.  
이 버려진 데이터그램은 송신 측에서 손실 이벤트를 발생시키고, 송신자는 송신자와 수신자 사이의 경로상 혼잡이 발생했음을 알게 된다.  
<br>

마지막으로 TCP 송신자는 어떻게 자신이 송신할 속도를 결정하는가?의 문제가 남는데, 여기서 TCP 혼잡제어 알고리즘이 사용된다.  
이 알고리즘은 아래의 세가지 구성요소들을 가진다.  

1. slow start  
TCP 연결이 시작될 때, cwnd의 값은 일반적으로 1 MSS로 초기화되고, 그 결과 초기 전송률은 MSS/RTT가 된다.  
이때 TCP 송신자에게 가용 밴드폭은 MSS/RTT보다 훨씬 클 것이므로, TCP 송신자는 가용 밴드폭 양을 조속히 찾고자 한다.   
그러므로 slow start 상태에서는 확인응답을 받을때마다 지수적으로 혼잡 윈도우 크기를 증가시킨다.  
송신자는 지수적 증가를 손실 이벤트가 발생할 때까지 지속한다.  
<br>
만약 이전에 패킷 손실이 발생한 임계점이 존재한다면, 앞으로 네트워크 혼잡이 예상되므로 지수적으로 혼잡윈도우 크기를 늘리는 것은 비효율적일 것이다.  
따라서 이 순간부터 congestion avoidance로 진입한다.  
<br>

2. congestion avoidance  
혼잡 회피 상태로 들어가는 시점의 cwnd의 값은 대략 혼잡이 마지막으로 발견된 시점에서의 반이 된다.  
이 시점부터는 매 RTT 마다 cwnd의 값을 두 배로 하기보다는 TCP는 좀 더 보수적인 접근을 채택하여 RTT 마다 cwnd를 1씩 증가시킨다.  
<br>
만약 타임아웃이나 패킷 손실이 발생하면 송신자는 cwnd를 절반으로 줄이고, fast recovery 상태로 진입한다.  
<br>

3. fast recovery  
빠른 회복은 여러 방법이 존재하지만, 대표적으로 TCP Tahoe, TCP Reno가 존재한다.  
- TCP Tahoe : cwnd를 1MSS로 줄이고 다시 slow start 단계로 진입한다. 이때 congestion avoidance에서 혼잡이 발생해 절반으로 줄인 cwnd에 도달하면 다시 congestion avoidance로 진입한다.  
- TCP Reno : Reno는 congestion avoidance에서 절반으로 줄인 cwnd를 시작점으로 다시 congestion avoidance를 수행한다.


## 4. 네트워크 계층-데이터 평면
네트워크 계층은 서로 상호작용하는 '데이터 평면'과 '제여 평면'의 두 부분으로 나눌 수있다.  
데이터 평면의 역할은 '입력 링크'에서 '출력 링크'로 데이터그램을 전달하는 것이고, 제어 평면의 역할은 데이터그램이 송신 호스트에서 목적지 호스트까지 잘 전달되게끔 포워딩을 조정하는 것이다.  
<br>

### 포워딩과 라우팅
네트워크 계층의 근본적인 역할은 매우 단순한데, 송신 호스트에서 수신 호스트로 패킷을 전달하는 것이다.  
이를위한 두 가지 네트워크 계층의 중요 기능은 '포워딩'과 '라우팅'이다.  
- 포워딩 : 패킷이 라우터의 입력 링크에 도달했을 때 라우터는 그 패킷을 적절한 출력 링크로 이동시켜야 한다. 이는 데이터 평면에 구현된 기능이다.  
- 라우팅 : 송신자가 수신자에게 패킷을 전송할 때 네트워크 계층은 패킷 경로를 지정해야 한다. 이러한 경로를 계산하는 것을 '라우팅 알고리즘'이라고 부르며, 제어 평면에 구현된 기능이다.  

### 라우터 내부 구조
상위 레벨에서 본 일반적인 '라우터 구조'는 아래의 그림과 같고, 네가지 요소가 존재한다.  
<br>
<img width="1228" alt="image" src="https://user-images.githubusercontent.com/76645095/179399939-7417ba01-cd6d-4f8b-8522-963811dc595b.png">

<br>

1. 입력 포트  
입력 포트는 라우터로 들어오는 입력 링크의 물리 계층 기능을 가장 먼저 수행한다.  
또한 입력 링크의 반대편에 있는 링크 계층과 통신하기 위해 필요한 링크 계층 기능도 수행한다. 이것은 그림에서 입력 포트의 중간 상자가 수행한다고 생각하면 된다.  
가장 중요한 것은 입력 포트에서 '검색 기능'을 수행하는 것인데, 이는 가장 오른쪽 상자가 수행한다. 이때 포워딩 테이블을 참조하여, 도착된 패킷이 스위칭 구조를 통해 전달되는 라우터 출력 포트를 결정한다.  

2. 스위칭 구조  
스위칭 구조는 라우터의 입력 포트와 출력 포트를 연결한다.  
3. 출력 포트  
출력 포트는 스위칭 구조에서 수신한 패킷을 저장하고, 필요한 링크 계층 및 물리적 계층 기능을 수행하여 출력 링크로 패킷을 전송한다.  
또한 출력 포트는 일반적으로 동일한 링크의 입력 포트와 한 쌍을 이룬다.  

4. 라우팅 프로세서  
라우팅 프로세서는 '제어 평면 기능'을 수행한다. 기존의 라우터에서는 라우팅 프로토콜을 실행하고, 라우팅 테이블과 연결된 링크 상태 정보를 유지 관리하며 라우터의 포워딩 테이블을 계산했다.  
반면 SDN이 사용된 라우터에서는 라우팅 프로세서는 원격 컨트롤러와 통신하여 원격 컨트롤러에서 계산된 포워딩 테이블 항목을 수신하고, 라우터의 입력 포트에 이러한 항목을 설치한다. 

그림과 같이 라우터의 데이터 평면에 속하는 '입력 포트' '출력 포트' '스위칭 구조'는 거의 항상 하드웨어로 구현된다.  

### 패킷 스케줄링 
하나의 라우터에 패킷이 몰릴 수있고, 이때 패킷은 큐에서 출력 링크를 통해 전송되는 순서를 대기한다.  
이렇게 큐에서 대기중인 패킷들을 전송하는 순서를 결정하는 것이 '패킷 스케줄링'이며 크게 아래와 같은 방법들이 존재한다.
- FIFO (First-In-First-Out)
- 우선순위 큐잉
- Round robin
- WFQ

하나씩 살펴보자.

#### First-In-First-Out (FIFO)
아래의 그림은 FIFO 스케줄링의 큐 모델 개념도를 보여준다.  

<img width="747" alt="image" src="https://user-images.githubusercontent.com/76645095/179437500-e00f058f-a868-423f-9e5c-809cfc376b1b.png">
<br>

만약 회선이 현재 다른 패킷을 전송 중이라면, 링크 출력 큐에 도착한 패킷은 전송을 기다린다.   
그러다, 패킷이 출력되는 회선을 통해 완전히 전송되면 큐에서 제거되는 구조이다.  
<br>
FIFO(or FCFS) 스케줄링 규칙은 출력 링크 큐에 도착한 순서와 동일한 순서로 출력 링크에서 전송할 패킷을 선택한다.  
즉, 새롭게 도착한 패킷은 대기중인 줄 뒤쪽에서 기다리다가, 줄의 맨 앞으로 도착하면 서비스를 제공받는다.  

#### 우선순위 큐잉
아래의 그림은 우선순위 큐잉의 모델 개념도를 보여준다.  

<img width="980" alt="image" src="https://user-images.githubusercontent.com/76645095/179437988-e880d386-6be1-40c0-9756-a001829bcdf2.png">

우선순위 큐잉에서 출력 링크로 도착한 패킷은 위의 그림과 같이 큐에 도착하면 우선순위 클래스로 분류된다.  

각 우선순위 클래스에는 일반적으로 고유한 큐가 있으며, 전송할 패킷을 선택할 때 우선순위 큐는 전송대기 중인 패킷으로 차있는 상태이고, 가장 높은 우선순위 클래스에서 패킷을 전송한다.  
이때 동일한 우선순위 클래스에 속하는 패킷들 중 선택하는 것은 전형적인 FIFO 방식을 사용한다.
<br>

#### 라운드 로빈(Round Robin)과 WFQ(Weighted Fair Queuing)  
라운드 로빈 큐잉 규칙에서 패킷은 우선순위 큐잉과 같이 클래스로 분류된다.  
이때 클래스 간에는 엄격한 서비스 우선순위가 존재하지 않으며, 라운드 로빈 스케줄러가 클래스 간에 서비스를 번갈아서 제공한다.  
<br>
라우터에서 널리 구현된 라운드 로빈 큐잉의 일반적인 형태는 WFQ 규칙이며 아래의 그림과 같은 모델 개념도를 가진다.  

<img width="1015" alt="image" src="https://user-images.githubusercontent.com/76645095/179438598-1b462df8-7e24-4e9f-94a6-023efadf4cfe.png">

<br>

WFQ에서 도착하는 패킷은 적절한 클래스별 대기 영역에서 분류되며 대기한다.  
라운드 로빈 스케줄링과 같은 방식으로 동작하기 때문에 WFQ 스케줄러는 순환 방식으로 동작한다.  
<br>
class가 3개 있다고 가정했을 때, 먼저 class 1이 동작하고, class 2가 동작하고, class 3이 순서대로 동작하는 패턴을 반복한다.  
<br>
또한 WFQ는 작업 보존 큐잉 규칙이며, 빈 클래스 큐를 찾으면 서비스 순서에서 다음 클래스로 즉시 이동한다.  

하지만, 라운드 로빈과 WFQ는 약간의 차이가 존재하는데, WFQ는 각 클래스마다 다른 양의 서비스 시간을 부여 받는다는 점이다.  

### 서브넷이란?
서브넷은 간단하게 말하면 고랍된 네트워크를 말한다. 좀더 하드웨어적으로 말하면, 호스트의 인터페이스들과 '하나의 라우터 인터페이스'로 연결된 네트워크를 서브넷이라 부른다.  

### DHCP(Dynamic Host Configuration Protocol)
호스트에 IP 주소를 할당하는 것은 수동으로 구성이 가능하지만, 일반적으로 DHCP을 더 많이 사용한다.  
네트워크 관리자는 해당 호스트가 네트워크에 접속하고자 할 때마다 동일한 IP 주소를 받도록 하거나, 다른 임시 IP 주소를 할당하도록 DHCP를 설정한다.  
<br>
DHCP는 호스트 IP 주소를 할당할 뿐만 아니라, 서브넷 마스크, 첫번째 홉 라우터 주소, 로컬 DNS 서버 주소 같은 추가 정보를 얻게 해준다.  
<br>
DHCP는 호스트를 네트워크에 자동으로 연결해주는데, 이러한 능력 때문에 '플러그 앤 플레이(plugin and play)' 프로토콜이라고 불리기도 한다.  
또한 DHCP의 특성상 호스트가 빈번하게 접속하고 떠나는 가정 인터넷 접속 네트워크, 엔터프라이즈 네트워크, 무선 LAN에서도 폭넓게 사용한다.  
<br>

DHCP도 client-server protocol이다. 여기서 client는 네트워크 설정을 위한 정보를 얻고자 새롭게 도착한 호스트를 말한다.  
아래의 그림은 새로운 호스트가 도착할 경우, 네트워크 상에서 수행될 DHCP 프로토콜 4단계를 보여준다.  

<img width="964" alt="image" src="https://user-images.githubusercontent.com/76645095/179450533-f3242a10-646f-4aa7-82c6-3bebd8bdc593.png">
<br>

그림에서 yiaddr은 새롭게 도착한 클라이언트에게 할당될 주소를 나타낸다. 각 순서는 아래와 같이 진행된다.  

1. DHCP Discover  
먼저 새롭게 도착한 호스트는 상호 동작될 DHCP를 발견한다. 이것은 DHCP discover message를 사용하여 수행되며, 클라이언트는 포트 67번으로 UDP 패킷을 보낸다.  
그러나 client는 자신이 접속될 네트워크의 IP 주소를 모르는 상태이며, 네트워크의 DHCP IP도 모른다.  
<br>
따라서 이 메시지 내의 목적지 IP 주소를 브로드캐스팅 주소인 (255.255.255.255)로 설정하고, 출발지 IP 주소는 (0.0.0.0)으로 설정하여 링크 계층으로 패킷을 보낸다.  
2. DHCP Offer  
DHCP Discover 메시지를 받은 DHCP 서버는 DHCP Offer 메시지를 client로 응답한다.  
이때 client의 IP 주소가 없는 상태이므로, 브로드캐스트 주소 255.255.255.255를 사용하여 서브넷의 모든 노드로 이 메시지를 브로드캐스팅한다.  
3. DHCP Request  
client는 Discover 메시지를 브로드캐스트 방식으로 보내기 때문에 여러 DHCP 서버가 DHCP Offer 메시지를 보낼 수있다.  
클라이언트는 한개 이상의 DHCP Offer 메시지를 받고, 가장 최적에 위치한 DHCP 서버를 선택한다.  
그 후 DHCP Offer 메시지에 들어있는 파라미터를 선택하여 DHCP Request 메시지에 해당 정보를 넣는다.  
<br>
이때 DHCP Request message도 브로드캐스트 방식으로 전송한다.  
이유는 선택되지 않은 DHCP 서버들에게도 자신이 어떤 DHCP 서버를 선택했는지 알려줘야 하기 때문이다.  

4. DHCP ACK  
DHCP Request 메시지를 받은 DHCP 서버는 요청된 파라미터를 확인하고, DHCP ACK 메시지를 클라이언트에게 전송한다.  


이제 클라이언트는 DHCP ACK 메시지를 받고, yiaddr에 들어있는 IP 주소를 사용햘 수있다.  

